<!DOCTYPE HTML>
<html>
	<head>
		<title>Deciphering Big Data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Lucy Ringwood</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="big_data.html" class="active">Deciphering Big Data</a></li>
						
					</ul>
				</nav>
			</header>
		
		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="index.html">Home</a></li>
                           				<li><a href="#artefacts">Artefacts Created</a></li>
                            				<li><a href="#reflection">Reflective Piece</a></li>
                            				<li><a href="#skills_matrix">Professional Skills Matrix and Action Plan</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Deciphering Big Data</h1>
							<img src="data_pro.jpg" alt="" data-position="center center" />
							<h2> Learning Outcomes </h2>
							<p> 1. Demonstrate a critical awareness of roles and responsibilities of a Data Science Professional.</p>
							<p> 2. Critically analyse architecture, design, development methodology, querying and the lifecycle of managing large-scale datasets.</p>
							<p> 3. Distinguish between and critically reflect on the solutions of various data analytics approaches which support the business decision-making process.</p>
							<p> 4. Apply and evaluate critically the various methods, tools, technologies, and success factors applied to a data science project in order to develop an effective plan and delivery of solutions to a business problem.</p>
							
                            <!-- Artefacts Created -->
                            <section id="artefacts">
                                <h2 class="major">Artefacts Created</h2>
                                <p></p>
                            </section>

                            <!-- Collaborative Discussion -->
                            <section id="discussion">
                                <h2 class="major">Collaborative Discussion</h2>
                                <b class="major">Collaborative Discussion 1 - The Data Collection Process</b>
				<p> The Internet of Things (IoT) is a term that describes network of interconnected physical devices which communicate and share data without the need for human intervention. The basic theory behind IoT is that devices, when connected and able to easily exchange information, can provide enhanced efficiency, accuracy and economic benefits (Ashton, 2009). The volume and variety of data generated by IoT devices presents the potential for valuable business and personal insights. However, reaping those benefits is dependent on the quality of the data, which is often plagued by inconsistencies, missing values and inaccuracies. </p>

<p> Huxley delineates that big data, especially from sources like IoT, is susceptible to noise, missing values, and discrepancies (2020). These irregularities can be caused by differing specifications on devices, network interruptions or external environmental factors. The effectiveness of the data is likely to be compromised if these issues are not mitigated against. This can in turn skew subsequent analyses and result in misguided insights (Elkhodr et al, 2021). </p>

<p> To establish the potential of IoT data, a key stage is to employ thorough data cleaning methods. The methodologies discussed by Elkhodr et al. (2021) highlight techniques that not only rectify inaccuracies but also anticipate potential sources of error in IoT deployments. This is similar to Microsoft's Azure Architecture Data Guide, which provides strategies to accommodate vast and varied data sources (Microsoft, 2023). </p>

<p> Importance should also be placed on continual assessment and refinement of cleaning methodologies, ensuring that as IoT ecosystems evolve, so too do the strategies to ensure data integrity. </p>

<p> IoT presents a huge opportunity in the realm of big data, however, the principle of 'garbage in, garbage out' applies. There should be an emphasis on rigorous data cleaning, ensuring that the vast volumes of data generated by IoT devices are accurate, complete and reliable. As prevalence of IoT technologies continues to expand, the methodologies and tools employed to maintain data quality will play a significant role in realising its potential. </p>
<p> Ashton, K., 2009. That 'internet of things' thing. RFID Journal. Available at: https://www.rfidjournal.com/that-internet-of-things-thing. [Accessed 28 October 2023] </p>

<p> Elkhodr, M., Shahrestani, S. and Cheung, H., 2021. The IoT Data Management & Analysis Challenges: Storage, Process, Query, and Privacy. Available at: https://arxiv.org/ftp/arxiv/papers/2103/2103.13303.pdf  [Accessed 28 October 2023] </p>

<p> Huxley, K., 2020. Data Cleaning. In: Paul Atkinson, ed., Sage Research Methods Foundations. London: SAGE Publications Ltd. Available at: https://doi.org/10.4135/9781526421036842861 [Accessed 28 Oct 2023]. </p>

<p> Microsoft, 2023. Big Data: Azure Architecture Data Guide. Available at: https://learn.microsoft.com/en-us/azure/architecture/data-guide/big-data/  [Accessed 28 October 2023]. </p>


				    	
				<b class="major">Collaborative Discussion 2 - Comparing Compliance Laws</b>   
				    <p> The General Data Protection Regulation (GDPR) is a regulation that affects all European Union (EU) member states, as well as other countries that have access to data belonging to EU citizens. The Information Commissioner's Office (ICO) is the United Kingdom's independent authority created to uphold information rights and data protection. The GDPR and the ICO prioritise both the security of personal data but their guidance varies. </p>
					<p> GDPR's view on securing personal data is articulated in Article 5(1)(f), which emphasises that personal data should be processed appropriately, protecting against things such as unlawful processing, loss or destruction by using security measures and organisational guidelines (GDPR-info.eu, n.d). This provision inherently broad, allowing application to many varied forms of data. By not outlining explicit security measures, the GDPR entrusts the data controller and processor with the discretion to determine what might be appropriate in their context, as the variety of industries to which GDPR applies is incredibly diverse (GDPR Eu, n.d). </p>
					<p> In comparison, the ICO's "Security" principle resonates with the GDPR's ethos but also incorporates specifics tailored to the UK's unique landscape. The ICO explains that personal data must be shielded against unauthorised or unlawful processing, as well as accidental incidents that could lead to loss, damage or destruction (ICO, n.d). Similar to GDPR, this principle is applicable to all forms of data. However, the ICO offers tools, resources, and guidance that cater specifically to UK organisations. The ICO functions on the ‘data protection by design and default’ value, urging organisations to recognise the inherent risks to their data and then implement appropriate countermeasures (ICO, n.d). Supplementing this approach, the ICO provides a collection of resources including self-assessment instruments, guidelines on encryption and literature on the latest capabilities such as Privacy Enhancing Technologies (ICO, 2022). </p>

<p> GDPR-info.eu (n.d) Article 5 GDPR – Principles relating to processing of personal data. Available at: https://gdpr-info.eu/art-5-gdpr/. [Accessed: 23 October 2023] </p>
<p> GDPR EU (n.d) GDPR personal data – what information does this cover? Available at: https://www.gdpreu.org/the-regulation/key-concepts/personal-data/ [Accessed: 28 October 2023] </p>
<p> ICO (n.d.) Guide to the General Data Protection Regulation (GDPR). Available at: https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/ [Accessed: 23 October 2023] </p>
<p> ICO (n.d.) Guide to Accountability and Governance. Available at: https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/ [Accessed: 23 October 2023] </p>
<p> ICO (n.d.) A Guide to Data Security. Available at:  https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/security/a-guide-to-data-security/ [Accessed: 23 October 2023] </p>
<p> ICO (2022) Chapter 5:Privacy-Enhancing Technologies (PETs) Available at: https://ico.org.uk/media/about-the-ico/consultations/4021464/chapter-5-anonymisation-pets.pdf [Accessed: 23 October 2023] </p>


				<b class="major">Web Scraping</b>   
				    <p> The first stage of conducting web scraping was understanding the HTML code that underpins a website's structure and content. After grasping these concepts, I could then use this knowledge to inspect the HTML on my chosen website to use the Python library "Beautiful Soup". This library facilitates the  web scraping process by parsing the HTML, using tags/attributes to navigate the content. The find.all() method can search for matching strings and append them to a list for subsequent printing or saving to a preferred file type. I have scraped the words "data scientist" from the Data Science Wikipedia page.    </p>
                            	<p> My code can be found here: <a href="Web Scraping-Data Scientist.ipynb" download>Click here to download the Python script</a> </p>	
				    
				    
				<b class="major">APIs and Security Requirements</b>   
				    <p> Exploring Application Programming Interfaces (APIs) felt a natural progression from learning skills in webscraping as APIs offer more stability, efficienc and are often more in line with legal and ethical considerations. I expaned my understanding of how APIs operate and the measures needed to be protect data and devices when utlilising them. I wrote a piece of code that fetches data from one of NASA's open APIs. The data relates to the weather on Mars. I have included specifications which set the content type (JSON), check that the connection is secure (HTTPS), set the timeout to 10 seconds to mitigate indefinite processing if errors occurs and a check on any API rate limits which may be implemented by NASA.  </p>
                            	<p> My code can be found here: <a href="NASA API.ipynb" download>Click here to download the Python script</a> </p>	

				<b class="major">Normalisation</b>   
				    <p> <p> Learning about the First, Second, and Third Normal Forms (1NF, 2NF, and 3NF) in database design was a new concept to me. I gained an understanding of the importance of a structured and efficient database, which consequently reduces data redundancy and ensures data integrity. The process involves using Python to refine the database step by step. This understanding is crucial for making sensible business decisions in database design and prioritises the need for data integrity and, in turn, the performance of the database. </p>
                            	<p> My code can be found here: <a href="Normal Form.ipynb" download>Click here to download the Python script</a> </p>	
				    
			    </section>

                            <!-- Reflective Piece -->
                            <section id="reflection">
                                <h2 class="major">Reflections</h2>
					<p><span class="image left"><img src="images/pic05.jpg" alt="" /></span>  The Deciphering Big Data module advancedmy existing basic understanding of Python and provided an understanding of how professional databases are built and managed by large organisations.  The group project was related to developing a Database Management System for a train company who wanted a new system to manage customer contact and complaints. My responsibility was to contribute the Data Management Pipeline, Data Wrangling, and Compliance sections. </p>
<p> The scope of the project was quite vast and intimidating at first glance when I reviewed the content of the whole module during Unit 1. I am aware that I often feel “imposter syndrome”, which can cause me to spend more time worrying that I am not capable of complete tasks such as large assignments. I am cognisant that this is counter-productive and so actively tried to approach the tasks and assignments in this module in a sequential and step-by-step manner to limit feeling overwhelmed. </p>
<p> Once the teams were formed and we’d discussed the brief together, the group assignments seemed much more manageable than I had originally thought. As a team we discussed what the requirements of the assignment were and then volunteered to take responsibility for the sections where we felt our skills lay. In hindsight, although this contributed to a project, as a learning experience I could have volunteered to work on concepts which I felt I had less knowledge of in order to advance weaker skills (Leitner, 2016). </p>
<p> In the first assignment, “Project Report”, my focus was on data retrieval and pipeline. I learn best from practical exercises so, despite working code not being a requirement for the project, I took the time to research the API from National Rail Data Portal, utilising the Python “requests” library to execute a GET request and then captured the data in XML format ready for cleaning. These concepts were very new to me and it took a lot of reading to understand the processes and responsibilities of each step. Implementing the steps myself really helped me to grasp how capturing data via an API works. I felt a great sense of achievement when I executed the code successfully and retrieved the data. I really enjoyed the trial and error process, making incremental improvements until I achieved my aim. This gave me a lot more confidence in learning techniques that are new to me. I identified that the “ElementTree” library was suitable to parse and structure the data. I also explored the types of standardisation that would need be undertaken in order for the relational database to function with data from various sources. </p>
<p> I already had experience of manually cleaning data with Python, but the “Executive Summary” called for an approach on a much larger scale. This meant that I needed to explore more commercial tools, such as ETL processes with AWS Glue and event-driven Lamda functions. This helped me to understand how big data can be leveraged in a commercial landscape. Despite this technology being available, I was interested to learn that sometimes handwritten code can still be the best course of action to ensure succinct, efficient and accurate processes are implemented (AWS Workshops, n.d.). </p>
<p> Working on the compliance section reminded me of the critical importance of data security. I work in the law enforcement industry and therefore have a deep understanding of the importance of information security, but this element of the unit gave me a more technical understanding of concepts that underpin security, such as identity and access control, data backups and physical and environmental security (CompTIA, 2016).  The implementation of AWS' Identity and Access Management (IAM) in the project allowed me to gain an understanding of the commercial options available to implement this aspect of security. </p>
<p> Working as a team presented opportunities to advance collaboration skills and learn more about working remotely with those outside of my workplace. One key skill that I developed a greater understanding of was communication on timekeeping; this was key to ensuring a successful dynamic. We all communicated our commitments to make each other aware of availability for meetings and for when one team member’s work relied on another member’s being completed first. Clear checkpoints were set and meeting them enabled us to complete the project on time, without any last-minute stress. Regular communication in general really made the group dynamic easy. We mainly communicated by email, which allowed us to update each other on our own schedules, without needing to regularly find times that we were all available. It also allowed us to support and encourage each other on a regular basis. </p> 
<p><span class="image right"><img src="images/pic06.jpg" alt="" /></span> Upon reviewing my progress through the Deciphering Big Data module, several key points stand out. Firstly, the practical experience confirmed my belief that “learning by doing” is the best way for me, personally, to learn. In future modules and outside of academia, I aim to seek opportunities to put the concepts I learn into practice to solidify my learning. Whilst specialisation is often essential to stand out in the workplace, the team projects emphasised the importance of diversifying my capabilities to achieve a solid foundation in many areas, suggesting that volunteering for unfamiliar roles can sometimes offer the greatest benefits. </p>
<p> Another conclusion that I identified was the impact of "imposter syndrome" on my productivity and confidence. Recognising when this is occurring and taking active steps to combat negative feelings, such as reframing my thoughts, will be valuable in my future roles, ensuring that self-doubt doesn't inhibit my potential. I intend to seek out opportunities in the workplace for mentoring so I can gain support on an individual basis (Leitner, 2016). </p>
<p> The collaboration aspect, whilst largely a positive experience, showcased the potential for difficult situations had we not had clear communication. Moving forward, I'll actively suggest methods to enhance clearer communication channels and possibly implement Microsoft Teams chats for example to take advantage of when team members are working simultaneously. The respect for individual responsibilities and time constraints was a value that I believe is of utmost importance. </p>
<p> This module has provided me with a vital balance of theoretical, practical and personal growth, all of which contribute to the development of successful a data science professional. </p>

<p> AWS Workshops (n.d). AWS Glue Immersion day. Available at: https://catalog.us-east-1.prod.workshops.aws/workshops/ee59d21b-4cb8-4b3d-a629-24537cf37bb5/en-US/lab6/etl/advanced-transform [Accessed 21 October 2023]. </p>
<p> CompTIA (2016). Quick Start Guide to Security Compliance. Available at: https://www.comptia.org/content/guides/quick-start-guide-to-security-compliance [Accessed 28 October 2023] </p>
<p> Leitner, M (2016). Confronting the Reality of Impostor Syndrome in the Data and Analytics Industry. Towards Data Science. Available at: https://towardsdatascience.com/confronting-the-reality-of-impostor-syndrome-in-the-data-and-analytics-industry-30269dfcbc41 [Accessed 28 October 2023]. </p>

			  </section>

                          

                            <!-- Professional Skills Matrix and Action Plan (PDP) -->
                            <section id="skills_matrix">
                                <h2 class="major">Professional Skills Matrix and Action Plan (PDP)</h2>
                                <p>Placeholder content</p>
                            </section>
                            
						</div>
					</section>
			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
